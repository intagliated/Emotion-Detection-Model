<!--
# Emotion-Detection-Model

In this project, dataset provided by Kaggle website is used (https://www.kaggle.com/astraszab/facial-expression-dataset-image-folders-fer2013), which consists of about 35,887 well structured 48 × 48 pixel gray-scale images of faces. Each image has to be categorized into one of the seven classes that express different facial emotions. These facial emotions have been categorized as: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, and 6=Neutral. The given images are divided into three different sets which are training, validation, and test sets. There are about 28,709 training images, 3589 validation images, and 3589 images for testing. In order to classify the expressions, the features generated by convolution layers using the raw pixel data are used.

Here is your content structured into bullet points for a clean and readable **GitHub `README.md`** format:
'''
---
-->

```markdown
Teammates:
1. Maria Paul T
2. Rose Joseph
```


# Emotion-Detection-Model

* **Dataset Source:**
  Dataset used in this project is from Kaggle: [Facial Expression Dataset (FER-2013)](https://www.kaggle.com/astraszab/facial-expression-dataset-image-folders-fer2013)

* **Image Details:**

  * Total of **35,887** well-structured images
  * Each image is **48 × 48 pixels**, grayscale
  * Contains faces showing different facial expressions

* **Emotion Categories:**
  Each image is labeled with one of the **seven facial emotions**:

  * `0` = Angry
  * `1` = Disgust
  * `2` = Fear
  * `3` = Happy
  * `4` = Sad
  * `5` = Surprise
  * `6` = Neutral

* **Data Split:**
  The dataset is divided into three sets:

  * **Training Set:** 28,709 images
  * **Validation Set:** 3,589 images
  * **Test Set:** 3,589 images

* **Feature Extraction:**

  * Raw pixel data is passed through **convolutional layers**
  * Convolutional layers are applied to raw pixel data  
  * Extracted features are used to **classify facial expressions**

---

